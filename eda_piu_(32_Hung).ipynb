{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8c649a",
   "metadata": {},
   "source": [
    "# 1. Import library\n",
    "Import all necessary libraries throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903503c5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-18T13:42:47.082649Z",
     "iopub.status.busy": "2024-12-18T13:42:47.082342Z",
     "iopub.status.idle": "2024-12-18T13:43:05.809103Z",
     "shell.execute_reply": "2024-12-18T13:43:05.808428Z"
    },
    "papermill": {
     "duration": 18.741682,
     "end_time": "2024-12-18T13:43:05.811133",
     "exception": false,
     "start_time": "2024-12-18T13:42:47.069451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.base import clone, BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c984b1",
   "metadata": {},
   "source": [
    "### Helper function\n",
    "These functions help us to read and preprocess parquet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037b3281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:43:05.830019Z",
     "iopub.status.busy": "2024-12-18T13:43:05.829364Z",
     "iopub.status.idle": "2024-12-18T13:43:05.835437Z",
     "shell.execute_reply": "2024-12-18T13:43:05.834656Z"
    },
    "papermill": {
     "duration": 0.0169,
     "end_time": "2024-12-18T13:43:05.836946",
     "exception": false,
     "start_time": "2024-12-18T13:43:05.820046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    \"\"\"\n",
    "    Reads a Parquet file, processes its contents,\n",
    "    and returns n time series value extracted from the data and an id of a volunteer.\n",
    "\n",
    "    Parameters:\n",
    "        dirname (str): The directory path where the file is located.\n",
    "        filename (str): The filename of the Parquet file to be read. The file is expected to be in a subdirectory\n",
    "                        named after the `filename` parameter, containing a part file named 'part-0.parquet'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - numpy.ndarray: Flattened time series data of the DataFrame (excluding the 'step' column).\n",
    "            - str: A substring extracted from the `filename`, split by '=' - this is an ID of the volunteer\n",
    "             and returning the second part.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "      Loads and preprocesses time series data from multiple files in a directory,\n",
    "      returning a DataFrame containing n time series features for each volunteer.\n",
    "\n",
    "      Parameters:\n",
    "          dirname (str): The directory path containing the time series files to preprocess.\n",
    "\n",
    "      Returns:\n",
    "          pd.DataFrame: A DataFrame with the following structure:\n",
    "              - Columns `stat_0`, `stat_1`, ..., `stat_n`: n time series features extracted from each file.\n",
    "              - Column `id`: The unique identifiers (derived from filenames) for each volunteer.\n",
    "    \"\"\"\n",
    "    ids = os.listdir(dirname)\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "\n",
    "    stats, indexes = zip(*results)\n",
    "\n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4dea2",
   "metadata": {
    "papermill": {
     "duration": 0.008224,
     "end_time": "2024-12-18T13:43:05.853450",
     "exception": false,
     "start_time": "2024-12-18T13:43:05.845226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Read data\n",
    "This section is the data loading CSV and time series data, extract time series data part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39e2dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:43:05.871083Z",
     "iopub.status.busy": "2024-12-18T13:43:05.870805Z",
     "iopub.status.idle": "2024-12-18T13:44:20.559115Z",
     "shell.execute_reply": "2024-12-18T13:44:20.558233Z"
    },
    "papermill": {
     "duration": 74.699137,
     "end_time": "2024-12-18T13:44:20.560822",
     "exception": false,
     "start_time": "2024-12-18T13:43:05.861685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:14<00:00, 13.39it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reading training and test data (CSV)\n",
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "dict = pd.read_csv('../input/child-mind-institute-problematic-internet-use/data_dictionary.csv')\n",
    "\n",
    "# Reading and preprocessing time series data from .parquet file\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd660a",
   "metadata": {
    "papermill": {
     "duration": 0.022793,
     "end_time": "2024-12-18T13:44:20.606814",
     "exception": false,
     "start_time": "2024-12-18T13:44:20.584021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.Preprocess data\n",
    "We perform preprocessing for both CSV data and time series data. Specifically, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801de77d",
   "metadata": {
    "papermill": {
     "duration": 0.022678,
     "end_time": "2024-12-18T13:44:20.651979",
     "exception": false,
     "start_time": "2024-12-18T13:44:20.629301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1 Preprocess csv\n",
    "We perform some techniques as mentioned in the presentation to preprocess CSV data, such as:\n",
    "* We remove columns that exist in the training data but not in the testing data.\n",
    "* We drop Nan Label.\n",
    "* We remove columns with more than 70% missing data (Nan value).\n",
    "* We handle columns ('season' features) with string values.\n",
    "* In addition, we generate new important features (by new method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcfa5b98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:20.698641Z",
     "iopub.status.busy": "2024-12-18T13:44:20.698314Z",
     "iopub.status.idle": "2024-12-18T13:44:20.705579Z",
     "shell.execute_reply": "2024-12-18T13:44:20.704732Z"
    },
    "papermill": {
     "duration": 0.03249,
     "end_time": "2024-12-18T13:44:20.707239",
     "exception": false,
     "start_time": "2024-12-18T13:44:20.674749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "nan_columns = test.columns[test.isna().mean() > threshold]\n",
    "# Remove columns with too many NaN values\n",
    "common_columns = train.columns.intersection(test.columns).difference(nan_columns)\n",
    "\n",
    "# Or not\n",
    "# common_columns = train.columns.intersection(test.columns) # Get the common columns between train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "877e2042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:20.754508Z",
     "iopub.status.busy": "2024-12-18T13:44:20.754218Z",
     "iopub.status.idle": "2024-12-18T13:44:20.762536Z",
     "shell.execute_reply": "2024-12-18T13:44:20.761843Z"
    },
    "papermill": {
     "duration": 0.034033,
     "end_time": "2024-12-18T13:44:20.764055",
     "exception": false,
     "start_time": "2024-12-18T13:44:20.730022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create data containing only the columns from the test set\n",
    "train_df = train[common_columns]\n",
    "\n",
    "# Reattach the label\n",
    "train_df['sii'] = train['sii']\n",
    "train = train_df\n",
    "\n",
    "test = test[common_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e8313",
   "metadata": {
    "papermill": {
     "duration": 0.022123,
     "end_time": "2024-12-18T13:44:20.808615",
     "exception": false,
     "start_time": "2024-12-18T13:44:20.786492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Encode Season\n",
    "By using the season_encode helper function - which converts string values ​​to categorical (int) form, we enable decision tree models to learn string features like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e34656f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:20.854108Z",
     "iopub.status.busy": "2024-12-18T13:44:20.853809Z",
     "iopub.status.idle": "2024-12-18T13:44:20.859463Z",
     "shell.execute_reply": "2024-12-18T13:44:20.858693Z"
    },
    "papermill": {
     "duration": 0.030325,
     "end_time": "2024-12-18T13:44:20.860950",
     "exception": false,
     "start_time": "2024-12-18T13:44:20.830625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def season_encode(df, kill_season=False):\n",
    "    \"\"\"\n",
    "    Encodes seasonal data in a DataFrame or removes seasonal columns based on input parameters.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data to process.\n",
    "        kill_season (bool, optional):\n",
    "            - If `True`, removes all columns with \"Season\" in their names.\n",
    "            - If `False`, encodes string columns with season-related data using a predefined mapping.\n",
    "            Default is `False`.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            - If `kill_season=True`, a DataFrame with \"Season\" columns removed.\n",
    "            - If `kill_season=False`, a DataFrame with seasonal data encoded and the `id` column preserved.\n",
    "\n",
    "    Mapping:\n",
    "        The seasonal strings are encoded as follows:\n",
    "        - 'Spring' -> 1\n",
    "        - 'Summer' -> 2\n",
    "        - 'Fall'   -> 3\n",
    "        - 'Winter' -> 4\n",
    "        - NaN      -> 0\n",
    "    \"\"\"\n",
    "    if kill_season:\n",
    "        season_cols = [col for col in df.columns if 'Season' in col]\n",
    "        df_ = df.drop(season_cols, axis=1)\n",
    "        return df_\n",
    "\n",
    "    df_no_id = df.drop(columns='id')\n",
    "    string_columns = df_no_id.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    season_encode_map = {\n",
    "        'Spring': 1,\n",
    "        'Summer': 2,\n",
    "        'Fall': 3,\n",
    "        'Winter': 4,\n",
    "        np.nan: 0\n",
    "    }\n",
    "\n",
    "    # Apply mapping for all string format columns\n",
    "    df_no_id[string_columns] = df_no_id[string_columns].apply(lambda col: col.map(season_encode_map))\n",
    "    df_no_id['id'] = df['id']\n",
    "    return df_no_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58250ea",
   "metadata": {
    "papermill": {
     "duration": 0.023174,
     "end_time": "2024-12-18T13:44:20.906749",
     "exception": false,
     "start_time": "2024-12-18T13:44:20.883575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature Engineering\n",
    "Based on strong correlations between features in the data, thereby creating features that are more meaningful to prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad3896c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:20.952617Z",
     "iopub.status.busy": "2024-12-18T13:44:20.952305Z",
     "iopub.status.idle": "2024-12-18T13:44:20.958910Z",
     "shell.execute_reply": "2024-12-18T13:44:20.958238Z"
    },
    "papermill": {
     "duration": 0.031339,
     "end_time": "2024-12-18T13:44:20.960429",
     "exception": false,
     "start_time": "2024-12-18T13:44:20.929090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df_):\n",
    "    \"\"\"\n",
    "    Performs feature engineering on a given DataFrame by creating new derived features\n",
    "    related to physical health, body composition, and internet usage.\n",
    "\n",
    "    Parameters:\n",
    "        df_ (pd.DataFrame): The input DataFrame containing the necessary columns for feature calculations.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the original columns and additional engineered features.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_.copy()\n",
    "    # Product of BMI and Age\n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    # Product of daily internet hours and Age\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    # Product of BMI and daily internet hours\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "\n",
    "    # Ratio of body fat percentage to BMI\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    # Ratio of fat-free mass index to body fat percentage\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    # Ratio of fat mass index to body fat percentage\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    # Ratio of lean soft tissue to total body water\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    # Product of body fat percentage and basal metabolic rate\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    # Product of body fat percentage and daily energy expenditure\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    # Ratio of basal metabolic rate to weight\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    # Ratio of daily energy expenditure to weight\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    # Ratio of skeletal muscle mass to height\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    # Ratio of skeletal muscle mass to fat mass index\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    # Ratio of total body water to weight\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    # Ratio of intracellular water\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    # Product of BMI and heart rate\n",
    "    df['BMI_PHR'] = df['Physical-BMI'] * df['Physical-HeartRate']\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b750c2c",
   "metadata": {
    "papermill": {
     "duration": 0.023293,
     "end_time": "2024-12-18T13:44:21.005918",
     "exception": false,
     "start_time": "2024-12-18T13:44:20.982625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2 Preprocess parquet (time series)\n",
    "In this section, we will learn how to extract and process time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e5b06",
   "metadata": {
    "papermill": {
     "duration": 0.022239,
     "end_time": "2024-12-18T13:44:21.050347",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.028108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Helper function auto encoder\n",
    "This function helps us automatically extract features of time series data using the AutoEncoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63720ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:21.096760Z",
     "iopub.status.busy": "2024-12-18T13:44:21.096462Z",
     "iopub.status.idle": "2024-12-18T13:44:21.102474Z",
     "shell.execute_reply": "2024-12-18T13:44:21.101646Z"
    },
    "papermill": {
     "duration": 0.031316,
     "end_time": "2024-12-18T13:44:21.104097",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.072781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network-based autoencoder for dimensionality reduction and feature extraction.\n",
    "\n",
    "    The autoencoder consists of an encoder and a decoder:\n",
    "    - The encoder compresses the input data into a lower-dimensional latent space.\n",
    "    - The decoder reconstructs the input data from the compressed representation.\n",
    "\n",
    "    Attributes:\n",
    "        encoder (nn.Sequential): A feedforward neural network that maps input data\n",
    "            to a lower-dimensional encoding using a series of linear layers and ReLU activations.\n",
    "        decoder (nn.Sequential): A feedforward neural network that reconstructs the input data\n",
    "            from the encoded representation using a series of linear layers and ReLU/Sigmoid activations.\n",
    "\n",
    "    Parameters:\n",
    "        input_dim (int): The dimensionality of the input data.\n",
    "        encoding_dim (int): The dimensionality of the latent space (encoded representation).\n",
    "\n",
    "    Methods:\n",
    "        forward(x):\n",
    "            Passes the input data through the encoder and decoder to produce reconstructed output.\n",
    "\n",
    "            Parameters:\n",
    "                x (torch.Tensor): The input data tensor with shape (batch_size, input_dim).\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The reconstructed data tensor with shape (batch_size, input_dim).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*2, input_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4922c51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:21.152663Z",
     "iopub.status.busy": "2024-12-18T13:44:21.152353Z",
     "iopub.status.idle": "2024-12-18T13:44:21.159107Z",
     "shell.execute_reply": "2024-12-18T13:44:21.158338Z"
    },
    "papermill": {
     "duration": 0.033375,
     "end_time": "2024-12-18T13:44:21.160667",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.127292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Performs dimensionality reduction using an autoencoder on the given DataFrame.\n",
    "\n",
    "    This function scales the input data, trains an autoencoder to compress the data\n",
    "    into a lower-dimensional space, and returns the encoded representation.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input data to be encoded.\n",
    "        encoding_dim (int, optional): The dimensionality of the latent space (encoded representation). Default is 50.\n",
    "        epochs (int, optional): The number of training epochs. Default is 50.\n",
    "        batch_size (int, optional): The size of each mini-batch during training. Default is 32.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the encoded representation with column names `Enc_1, Enc_2, ...`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scale the input data to standardize features\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "    # Convert scaled data into a PyTorch tensor\n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "\n",
    "    # Initialize the autoencoder with input dimensions and encoding dimensions\n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "\n",
    "    # Define the loss function (Mean Squared Error) and the optimizer (Adam)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "\n",
    "    # Train the autoencoder model\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            # Get the current mini-batch\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            reconstructed = autoencoder(batch)  # Forward pass\n",
    "            loss = criterion(reconstructed, batch)  # Compute reconstruction loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "        # Print the loss every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
    "\n",
    "    # Encode the data using the trained encoder\n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "\n",
    "    # Create a DataFrame for the encoded data\n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "\n",
    "    return df_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621b11d",
   "metadata": {
    "papermill": {
     "duration": 0.022538,
     "end_time": "2024-12-18T13:44:21.205910",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.183372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Encode data\n",
    "Perform data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bb630c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:21.251991Z",
     "iopub.status.busy": "2024-12-18T13:44:21.251675Z",
     "iopub.status.idle": "2024-12-18T13:44:21.258851Z",
     "shell.execute_reply": "2024-12-18T13:44:21.257983Z"
    },
    "papermill": {
     "duration": 0.032321,
     "end_time": "2024-12-18T13:44:21.260493",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.228172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encode\n",
      "Test encode\n"
     ]
    }
   ],
   "source": [
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "\n",
    "print(\"Train encode\")\n",
    "# train_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n",
    "train_ts_encoded = df_train\n",
    "print(\"Test encode\")\n",
    "test_ts_encoded = df_test\n",
    "# test_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2486f7",
   "metadata": {
    "papermill": {
     "duration": 0.022233,
     "end_time": "2024-12-18T13:44:21.305886",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.283653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loss ở test không giảm, không cần quá nhiều epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12e9e207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:21.352754Z",
     "iopub.status.busy": "2024-12-18T13:44:21.352417Z",
     "iopub.status.idle": "2024-12-18T13:44:21.357957Z",
     "shell.execute_reply": "2024-12-18T13:44:21.357221Z"
    },
    "papermill": {
     "duration": 0.031544,
     "end_time": "2024-12-18T13:44:21.359728",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.328184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reattach id column\n",
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]\n",
    "\n",
    "# Get all time series columns\n",
    "time_series_cols = train_ts_encoded.columns.tolist()\n",
    "time_series_cols.remove(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c172cb17",
   "metadata": {
    "papermill": {
     "duration": 0.024653,
     "end_time": "2024-12-18T13:44:21.407812",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.383159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.3 Combine data\n",
    "We perform the concatenation of the encoded string data with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c64e83b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:21.455016Z",
     "iopub.status.busy": "2024-12-18T13:44:21.454732Z",
     "iopub.status.idle": "2024-12-18T13:44:21.476262Z",
     "shell.execute_reply": "2024-12-18T13:44:21.475205Z"
    },
    "papermill": {
     "duration": 0.047924,
     "end_time": "2024-12-18T13:44:21.478564",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.430640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_combine = pd.merge(train, train_ts_encoded, how='left', on='id')\n",
    "test_combine = pd.merge(test, test_ts_encoded, how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d4cba3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:21.530678Z",
     "iopub.status.busy": "2024-12-18T13:44:21.529914Z",
     "iopub.status.idle": "2024-12-18T13:44:21.601685Z",
     "shell.execute_reply": "2024-12-18T13:44:21.600945Z"
    },
    "papermill": {
     "duration": 0.100164,
     "end_time": "2024-12-18T13:44:21.603827",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.503663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_combine = train_combine.drop('id', axis=1)\n",
    "test_combine = test_combine.drop('id', axis=1)   \n",
    "# Selecting feature (Drop features, which have >70% Nan values)\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "# Merge features from csv and time series features\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train_combine = train_combine[featuresCols]\n",
    "train_combine = train_combine.dropna(subset='sii')\n",
    "# Encode season data\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "        \n",
    "train_combine = update(train_combine)\n",
    "test_combine = update(test_combine)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train_combine)\n",
    "    mappingTe = create_mapping(col, test_combine)\n",
    "    \n",
    "    train_combine[col] = train_combine[col].replace(mapping).astype(int)\n",
    "    test_combine[col] = test_combine[col].replace(mappingTe).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58b7d61",
   "metadata": {
    "papermill": {
     "duration": 0.024377,
     "end_time": "2024-12-18T13:44:21.654413",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.630036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resolve NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed2f86a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:21.704633Z",
     "iopub.status.busy": "2024-12-18T13:44:21.703826Z",
     "iopub.status.idle": "2024-12-18T13:44:21.722100Z",
     "shell.execute_reply": "2024-12-18T13:44:21.721257Z"
    },
    "papermill": {
     "duration": 0.045061,
     "end_time": "2024-12-18T13:44:21.723748",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.678687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2736 entries, 0 to 3958\n",
      "Columns: 155 entries, Basic_Demos-Enroll_Season to stat_95\n",
      "dtypes: float64(143), int64(12)\n",
      "memory usage: 3.3 MB\n"
     ]
    }
   ],
   "source": [
    "train_combine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "352df82a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:21.824470Z",
     "iopub.status.busy": "2024-12-18T13:44:21.823840Z",
     "iopub.status.idle": "2024-12-18T13:44:21.833421Z",
     "shell.execute_reply": "2024-12-18T13:44:21.832596Z"
    },
    "papermill": {
     "duration": 0.035867,
     "end_time": "2024-12-18T13:44:21.835135",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.799268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sii\n",
       "0.0    1594\n",
       "1.0     730\n",
       "2.0     378\n",
       "3.0      34\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Presentating distribution of label after resolve NaN\n",
    "train_combine['sii'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaec57e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:21.882114Z",
     "iopub.status.busy": "2024-12-18T13:44:21.881867Z",
     "iopub.status.idle": "2024-12-18T13:44:21.888621Z",
     "shell.execute_reply": "2024-12-18T13:44:21.887702Z"
    },
    "papermill": {
     "duration": 0.032284,
     "end_time": "2024-12-18T13:44:21.890067",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.857783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sii\n",
       "0.0    1594\n",
       "1.0     730\n",
       "2.0     378\n",
       "3.0      34\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Presentating distribution of label before resolve NaN\n",
    "train['sii'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40d42137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:21.936329Z",
     "iopub.status.busy": "2024-12-18T13:44:21.936050Z",
     "iopub.status.idle": "2024-12-18T13:44:21.940971Z",
     "shell.execute_reply": "2024-12-18T13:44:21.940175Z"
    },
    "papermill": {
     "duration": 0.029864,
     "end_time": "2024-12-18T13:44:21.942609",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.912745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2736, 155)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_combine.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae3952",
   "metadata": {
    "papermill": {
     "duration": 0.022984,
     "end_time": "2024-12-18T13:44:21.988842",
     "exception": false,
     "start_time": "2024-12-18T13:44:21.965858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Train model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c885aa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:22.036632Z",
     "iopub.status.busy": "2024-12-18T13:44:22.035985Z",
     "iopub.status.idle": "2024-12-18T13:44:22.042571Z",
     "shell.execute_reply": "2024-12-18T13:44:22.041853Z"
    },
    "papermill": {
     "duration": 0.032363,
     "end_time": "2024-12-18T13:44:22.044134",
     "exception": false,
     "start_time": "2024-12-18T13:44:22.011771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "N_SPLITS = 5\n",
    "SEED = 42\n",
    "\n",
    "# Parameter for 3 model\n",
    "LightGBM_Params = {\n",
    "    'random_state': SEED, \n",
    "    'verbose':-1,\n",
    "    'n_estimators': 300,\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,\n",
    "    'lambda_l2': 0.01,\n",
    "    'device': 'cpu',\n",
    "}\n",
    "\n",
    "\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,\n",
    "    'reg_lambda': 5,\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'gpu_hist',\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,\n",
    "    'task_type': 'GPU'\n",
    "}\n",
    "\n",
    "RF_Params = {\n",
    "    'n_estimators': 200,             # Số lượng cây\n",
    "    'max_depth': 10,                 # Độ sâu tối đa của cây (trung bình của 6-12)\n",
    "    'min_samples_split': 5,          # Số mẫu tối thiểu để chia nhánh\n",
    "    'min_samples_leaf': 3,           # Số mẫu tối thiểu trên một lá\n",
    "    'max_features': 0.8,             # Tỷ lệ chọn thuộc tính tại mỗi node\n",
    "    'max_samples': 0.8,              # Tỷ lệ mẫu sử dụng để xây dựng mỗi cây\n",
    "    'random_state': SEED,            # Khóa ngẫu nhiên\n",
    "    'bootstrap': True,               # Bagging (tương đương với subsample)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e85c9021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:22.095938Z",
     "iopub.status.busy": "2024-12-18T13:44:22.095314Z",
     "iopub.status.idle": "2024-12-18T13:44:22.100687Z",
     "shell.execute_reply": "2024-12-18T13:44:22.099896Z"
    },
    "papermill": {
     "duration": 0.031363,
     "end_time": "2024-12-18T13:44:22.102499",
     "exception": false,
     "start_time": "2024-12-18T13:44:22.071136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Useful function\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa between the true labels and predicted labels.\n",
    "\n",
    "    Quadratic weighted kappa is a metric that measures the agreement between two categorical variables\n",
    "    while penalizing disagreements based on the magnitude of the difference.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (array-like): The true labels.\n",
    "        y_pred (array-like): The predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        float: The quadratic weighted kappa score between 0 (no agreement) and 1 (perfect agreement).\n",
    "    \"\"\"\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    \"\"\"\n",
    "    Rounds the continuous predictions to discrete classes based on specified thresholds.\n",
    "\n",
    "    This function takes a continuous set of predictions and rounds them to the nearest class\n",
    "    by comparing them against predefined threshold values.\n",
    "\n",
    "    Parameters:\n",
    "        oof_non_rounded (array-like): The continuous predictions to be rounded.\n",
    "        thresholds (list or array-like): The threshold values for classifying the predictions.\n",
    "            The thresholds should define the boundaries between classes.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: An array of rounded predictions (class labels).\n",
    "    \"\"\"\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of predictions by rounding them based on thresholds and calculating\n",
    "    the negative quadratic weighted kappa score.\n",
    "\n",
    "    This function rounds the predictions using the `threshold_Rounder` function and then calculates\n",
    "    the quadratic weighted kappa between the rounded predictions and the true labels.\n",
    "\n",
    "    Parameters:\n",
    "        thresholds (list or array-like): The threshold values for classifying the predictions.\n",
    "        y_true (array-like): The true labels.\n",
    "        oof_non_rounded (array-like): The continuous predictions to be rounded.\n",
    "\n",
    "    Returns:\n",
    "        float: The negative quadratic weighted kappa score between the true labels and the rounded predictions.\n",
    "    \"\"\"\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea88092c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:22.157257Z",
     "iopub.status.busy": "2024-12-18T13:44:22.156919Z",
     "iopub.status.idle": "2024-12-18T13:44:22.167697Z",
     "shell.execute_reply": "2024-12-18T13:44:22.166774Z"
    },
    "papermill": {
     "duration": 0.04098,
     "end_time": "2024-12-18T13:44:22.169710",
     "exception": false,
     "start_time": "2024-12-18T13:44:22.128730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Train and get predict function\n",
    "def train_predict(model, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Trains a model using Stratified K-Fold cross-validation, evaluates the performance using\n",
    "    quadratic weighted kappa (QWK) score, and makes predictions on the test data.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Splits the training data into K folds and trains the model on each fold.\n",
    "    2. Evaluates the model on both training and validation sets using quadratic weighted kappa.\n",
    "    3. Makes predictions on the test data and aggregates the results from all folds.\n",
    "    4. Optimizes thresholds for classification using a quadratic weighted kappa score.\n",
    "    5. Returns the final predictions and the trained model.\n",
    "\n",
    "    Parameters:\n",
    "        model (sklearn.base.Estimator): The model to be trained and evaluated.\n",
    "        train_data (pd.DataFrame): The training data containing features and the target label 'sii'.\n",
    "        test_data (pd.DataFrame): The test data to make predictions on.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the final predictions for the test data.\n",
    "        model: The trained model after the last fold.\n",
    "        float: The average validation QWK score across all folds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Align train and test input data\n",
    "    X = train_data.drop(columns=['sii'])\n",
    "    y = train_data['sii']\n",
    "\n",
    "    # Define K-Fold cross-validation\n",
    "    SKF = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    train_his = []\n",
    "    val_his = []\n",
    "\n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float)\n",
    "    oof_rounded = np.zeros(len(y), dtype=int)\n",
    "    test_preds = np.zeros((len(test_data), N_SPLITS))\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(tqdm(SKF.split(X, y), desc=\"Train progress\", total = N_SPLITS)):\n",
    "        # Determine the data for the fold\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Train the model\n",
    "        model_ = clone(model)  # Clone the model to ensure independence at each fold\n",
    "        model_.fit(X_train, y_train)\n",
    "\n",
    "        # Compute errors\n",
    "        y_train_pred = model_.predict(X_train)\n",
    "        y_val_pred = model_.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[val_index] = y_val_pred\n",
    "        y_train_pred_rounded = y_train_pred.round(0).astype(int)\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[val_index] = y_val_pred_rounded\n",
    "\n",
    "        # Evaluate the model performance\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred_rounded)\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_his.append(train_kappa)\n",
    "        val_his.append(val_kappa)\n",
    "\n",
    "        test_preds[:, fold] = model_.predict(test_data)\n",
    "\n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_his):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(val_his):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded),\n",
    "                              method='Nelder-Mead')\n",
    "\n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission, model_, np.mean(val_his)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2fa67",
   "metadata": {
    "papermill": {
     "duration": 0.024036,
     "end_time": "2024-12-18T13:44:22.416105",
     "exception": false,
     "start_time": "2024-12-18T13:44:22.392069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.1 XGBoost + LightGBM + CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a6724c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:22.463873Z",
     "iopub.status.busy": "2024-12-18T13:44:22.463587Z",
     "iopub.status.idle": "2024-12-18T13:44:22.471305Z",
     "shell.execute_reply": "2024-12-18T13:44:22.470548Z"
    },
    "papermill": {
     "duration": 0.034037,
     "end_time": "2024-12-18T13:44:22.472971",
     "exception": false,
     "start_time": "2024-12-18T13:44:22.438934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model instances\n",
    "LightGBM_Model = LGBMRegressor(**LightGBM_Params)\n",
    "XGBoost_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "# Combine models using Voting Regressor\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', LightGBM_Model),\n",
    "    ('xgboost', XGBoost_Model),\n",
    "    ('catboost', CatBoost_Model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6743c2fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:22.523060Z",
     "iopub.status.busy": "2024-12-18T13:44:22.522091Z",
     "iopub.status.idle": "2024-12-18T13:44:45.676267Z",
     "shell.execute_reply": "2024-12-18T13:44:45.675263Z"
    },
    "papermill": {
     "duration": 23.181724,
     "end_time": "2024-12-18T13:44:45.679562",
     "exception": false,
     "start_time": "2024-12-18T13:44:22.497838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress:  20%|██        | 1/5 [00:04<00:19,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train QWK: 0.7642, Validation QWK: 0.3778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress:  40%|████      | 2/5 [00:09<00:13,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Train QWK: 0.7652, Validation QWK: 0.4292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress:  60%|██████    | 3/5 [00:13<00:09,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Train QWK: 0.7712, Validation QWK: 0.4080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress:  80%|████████  | 4/5 [00:18<00:04,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Train QWK: 0.7810, Validation QWK: 0.3452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|██████████| 5/5 [00:22<00:00,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Train QWK: 0.7722, Validation QWK: 0.3984\n",
      "Mean Train QWK --> 0.7707\n",
      "Mean Validation QWK ---> 0.3917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.455\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Submission1, model, val = train_predict(voting_model, train_combine, test_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e496dd9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:45.776405Z",
     "iopub.status.busy": "2024-12-18T13:44:45.776017Z",
     "iopub.status.idle": "2024-12-18T13:44:45.785360Z",
     "shell.execute_reply": "2024-12-18T13:44:45.784649Z"
    },
    "papermill": {
     "duration": 0.059165,
     "end_time": "2024-12-18T13:44:45.787296",
     "exception": false,
     "start_time": "2024-12-18T13:44:45.728131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sii\n",
       "1    10\n",
       "0     9\n",
       "2     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Submission1['sii'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50c921d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:45.933713Z",
     "iopub.status.busy": "2024-12-18T13:44:45.932737Z",
     "iopub.status.idle": "2024-12-18T13:44:45.939703Z",
     "shell.execute_reply": "2024-12-18T13:44:45.939003Z"
    },
    "papermill": {
     "duration": 0.059566,
     "end_time": "2024-12-18T13:44:45.941993",
     "exception": false,
     "start_time": "2024-12-18T13:44:45.882427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Submission1.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac185a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T13:44:46.037073Z",
     "iopub.status.busy": "2024-12-18T13:44:46.036792Z",
     "iopub.status.idle": "2024-12-18T13:44:46.047693Z",
     "shell.execute_reply": "2024-12-18T13:44:46.046814Z"
    },
    "papermill": {
     "duration": 0.061258,
     "end_time": "2024-12-18T13:44:46.049522",
     "exception": false,
     "start_time": "2024-12-18T13:44:45.988264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    1\n",
       "10  0087dd65    0\n",
       "11  00abe655    1\n",
       "12  00ae59c9    2\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Submission1"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 125.75024,
   "end_time": "2024-12-18T13:44:50.491092",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-18T13:42:44.740852",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
